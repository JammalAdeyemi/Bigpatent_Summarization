{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:44:37.971996: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Certain Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of the dataset you want to split as train and test\n",
    "TRAIN_TEST_SPLIT = 0.1\n",
    "\n",
    "# Maximum length of the input to the model\n",
    "MAX_INPUT_LENGTH = 1024  \n",
    "\n",
    "# Minimum length of the output by the model\n",
    "MIN_TARGET_LENGTH = 5  \n",
    "# Maximum length of the output by the model\n",
    "MAX_TARGET_LENGTH = 128  \n",
    "\n",
    "# Batch-size for training our model\n",
    "BATCH_SIZE = 8 \n",
    "\n",
    "# Learning-rate for training our model \n",
    "LEARNING_RATE = 2e-5  \n",
    "\n",
    "# Maximum number of epochs we will train the model for\n",
    "MAX_EPOCHS = 10  \n",
    "\n",
    "# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
    "MODEL_CHECKPOINT = \"t5-small\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "The dataset consists of BBC articles and accompanying single sentence summaries. Specifically, each article is prefaced with an introductory sentence (aka summary) which is professionally written, typically by the author of the article. That dataset has 226,711 articles divided into training (90%, 204,045), validation (5%, 11,332), and test (5%, 11,334) sets. The dataset is available for download from [here](https://www.kaggle.com/pariza/bbc-news-summary).\n",
    "\n",
    "The dataset has the following fields:\n",
    "\n",
    "* **document**: the original BBC article to me summarized\n",
    "* **summary**: the single sentence summary of the BBC article\n",
    "* **id**: ID of the document-summary pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (/Users/oabas/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['document', 'summary', 'id'],\n",
      "    num_rows: 204045\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset(\"xsum\", split=\"train\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204045, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset \n",
    "raw_datasets = raw_datasets.train_test_split(\n",
    "    train_size=TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset into train and test sets\n",
    "# train_size = int(len(raw_datasets) * TRAIN_TEST_SPLIT)\n",
    "# test_size = len(raw_datasets) - train_size\n",
    "# train_dataset, test_dataset = raw_datasets.train_test_split(\n",
    "#     test_size=test_size\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`len(raw_materials)` computes the length of the raw_materials dataset. `TRAIN_TEST_SPLIT` is a constant value that has been defined earlier, which represents the percentage of data that has to be used for training the model. \n",
    "\n",
    "`train_size` calculates the number of elements to be used for training by multiplying the length of the raw_materials dataset with the `TRAIN_TEST_SPLIT` value and typecasting it to an integer.\n",
    "\n",
    "`test_size` calculates the number of data points that will be used for testing by subtracting the `train_size` from the original length of the `raw_materials` dataset.\n",
    "\n",
    "Then the `train_test_split()` function is called on the `raw_materials` dataset using the `test_size` and `random_state` parameters. The `test_size` parameter specifies the size of the test dataset, while the `random_state` parameter shuffles the data randomly when split. The function returns two datasets, the train dataset and the test dataset, which are stored in the variables `train_dataset` and `test_dataset` respectively. \n",
    "\n",
    "Finally, the code splits the dataset into two parts: a training set and a test set, which can be used to train a machine learning model and evaluate its accuracy respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I initializes a `tokenizer` object with a pre-trained model specified in the `MODEL_CHECKPOINT` variable.\n",
    "\n",
    "Next, the code checks if the `MODEL_CHECKPOINT` variable is set to any of the T5 models: `\"t5-small\"`, `\"t5-base\"`, `\"t5-large\"`, `\"t5-3b\"`, `\"t5-11b\"`. If it is, the `prefix` variable is set to `\"summarize: \"`. Otherwise, `prefix` remains an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a function called `preprocess_function` that takes a dictionary of examples as input. The function first concatenates a prefix string to each document in the examples. It then tokenizes the concatenated documents into a format suitable for input to a model, using a pre-defined tokenizer with a maximum input length of `MAX_INPUT_LENGTH`. The function also tokenizes the summaries in the examples using the same tokenizer and a maximum target length of `MAX_TARGET_LENGTH,` but in \"target\" mode. The function then adds the summary labels to the tokenized model inputs and returns the resulting dictionary of model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33cffb04a1094523b5f030a8506edbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20404 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d937cb2a02047a88f41e5414c0c9942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20405 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
    "# tokenized_datasets = train_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will apply the function on all the elements of all the splits in dataset, so our training, validation and testing data will be preprocessed in one single command."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-09 20:47:01.371262: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training Sequence to Sequence models, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Thus, we use the `DataCollatorForSeq2Seq` provided by the Hugging Face Transformers library on our dataset. The `return_tensors='tf'` ensures that we get `tf.Tensor` objects back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle = True,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "test_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "    shuffle = False,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = (\n",
    "    tokenized_datasets[\"test\"].\n",
    "    shuffle(seed=42).\n",
    "    select(list(range(200))).\n",
    "    to_tf_dataset(\n",
    "        columns = [\"input_ids\", \"attention_mask\", \"labels\"],\n",
    "        shuffle = False,\n",
    "        batch_size = BATCH_SIZE,\n",
    "        collate_fn = data_collator,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code creates three TensorFlow datasets for a natural language processing task. The first dataset is the `training dataset` which shuffles the data and batches it using the specified batch size. The second dataset is the `test dataset` which does not shuffle the data and also batches it using the specified batch size. The final dataset is a `generation dataset` which selects a subset of the `test data`, shuffles that subset and uses it to generate text. All datasets contain columns for input IDs, attention masks and labels, and use the specified `data_collator` function to collate the data into a batch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and Compiling the the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "model.compile(optimizer=optimizer, loss=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating the model\n",
    "\n",
    "To evaluate our model on-the-fly while training, we will define `metric_fn` which will calculate the `ROUGE` score between the groud-truth and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp\n",
    "\n",
    "rouge_l = keras_nlp.metrics.RougeL()\n",
    "\n",
    "def metric_fn(eval_predictions):\n",
    "    predictions, labels = eval_predictions\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    for label in labels:\n",
    "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge_l(decoded_labels, decoded_predictions)\n",
    "    # We will print only the F1 score, you can use other aggregation metrics as well\n",
    "    result = {\"RougeL\": result[\"f1_score\"]}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1604, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 576, in minimize\n        grads_and_vars = self._compute_gradients(\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 634, in _compute_gradients\n        grads_and_vars = self._get_gradients(\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 510, in _get_gradients\n        grads = tape.gradient(loss, var_list, grad_loss)\n\n    TypeError: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m callbacks \u001b[39m=\u001b[39m [metric_callback]\n\u001b[1;32m     10\u001b[0m \u001b[39m# For now we will use our test set as our validation_data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     12\u001b[0m     train_dataset, validation_data\u001b[39m=\u001b[39;49mtest_dataset, \n\u001b[1;32m     13\u001b[0m     epochs\u001b[39m=\u001b[39;49mMAX_EPOCHS, callbacks\u001b[39m=\u001b[39;49mcallbacks\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0c/_mtbsk_x4238bbw22x165tlm0000gn/T/__autograph_generated_fileotgv4z2w.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1604\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1601\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n\u001b[1;32m   1603\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1604\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m   1606\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_metrics\u001b[39m.\u001b[39mupdate_state(y, y_pred, sample_weight)\n\u001b[1;32m   1607\u001b[0m \u001b[39m# Collect metrics to return\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/engine/training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1604, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 576, in minimize\n        grads_and_vars = self._compute_gradients(\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 634, in _compute_gradients\n        grads_and_vars = self._get_gradients(\n    File \"/Users/oabas/opt/anaconda3/envs/data_science/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 510, in _get_gradients\n        grads = tape.gradient(loss, var_list, grad_loss)\n\n    TypeError: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.\n"
     ]
    }
   ],
   "source": [
    "# Now we can finally start training our model!\n",
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(\n",
    "    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n",
    ")\n",
    "\n",
    "callbacks = [metric_callback]\n",
    "\n",
    "# For now we will use our test set as our validation_data\n",
    "model.fit(\n",
    "    train_dataset, validation_data=test_dataset, \n",
    "    epochs=MAX_EPOCHS, callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
